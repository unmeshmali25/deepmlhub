{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa5798a-9a9b-4c25-b990-2d8b7e0ba0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617c73c-0a61-4082-a65e-760d914a4187",
   "metadata": {},
   "source": [
    "# Checking CUDA Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af424a65-da47-46dd-a441-4813936c252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version : 2.7.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pytorch version : {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be823162-3116-4fe2-9c7a-f409386cb8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? : True\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available? : {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "736bc047-27be-462a-b3e2-a83324cd7e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version : 12.8\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA version : {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e73793a-27cb-43ea-9ca6-8fc5f2cbffc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs : 1\n",
      "GPU memory : 42.41 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of GPUs : {torch.cuda.device_count()}\")\n",
    "print(f\"GPU memory : {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6a6b8-191f-4b96-ac15-be7bbe25cce3",
   "metadata": {},
   "source": [
    "# Tensor Creation Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d15c5cb-da50-4eb1-ba80-4434b6f633e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_tensor = torch.randn(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2520197-f527-4265-bfc7-a3c07705279a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU tensor : \n",
      "tensor([[ 1.0463,  0.2168,  1.0765],\n",
      "        [ 0.8140,  1.1421, -0.7343],\n",
      "        [ 0.0550, -0.8853,  1.2807]])\n",
      "Device : cpu\n",
      "Data Type : torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(f\"CPU tensor : \\n{cpu_tensor}\")\n",
    "print(f\"Device : {cpu_tensor.device}\")\n",
    "print(f\"Data Type : {cpu_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932d6b8-7060-4bab-aa21-d2c56a0b5734",
   "metadata": {},
   "source": [
    "### Creating tensor directly on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b89dee5d-b476-49c7-96e0-78e940c340a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Tensor : \n",
      "tensor([[ 0.4631, -1.1864, -0.0991],\n",
      "        [-0.4888,  1.6592, -2.2065],\n",
      "        [-0.8809, -0.0747, -0.9419]], device='cuda:0')\n",
      "Device : cuda:0\n",
      "Data Type : torch.float32\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    gpu_tensor = torch.randn(3,3, device = 'cuda')\n",
    "    print(f\"GPU Tensor : \\n{gpu_tensor}\")\n",
    "    print(f\"Device : {gpu_tensor.device}\")\n",
    "    print(f\"Data Type : {gpu_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "840e9a73-61b9-4a7f-9a54-cc1da1c469c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate way to do that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69ddb32f-fc13-41bd-a1ad-9d5881e003d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "cpu_to_gpu = torch.randn(3,3).cuda()\n",
    "print(f\"Device : {cpu_to_gpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7484c048-b61b-420d-8bf2-290064b035d9",
   "metadata": {},
   "source": [
    "# Measure Device Transfer Overhead"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4d4236c-0783-4e11-b26f-df8fa48bba49",
   "metadata": {},
   "source": [
    "Questions to answer \n",
    "- How expensive is CPU-GPU Transfer? \n",
    "- Does tensor time scale linearly with tensor size?\n",
    "- Should you batch multiple small transfers or do them individually? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "795ec52c-ba23-4c6b-93bf-dff696ebd327",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [100, 1000, 10000, 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c9849-006d-447d-99f8-ad93e71f3e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size            CPU->GPU (ms)        GPU->CPU (ms)       \n",
      "100             0.1645               0.0489              \n",
      "1000            0.3083               2.6274              \n",
      "10000           22.9452              248.5583            \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Size':<15} {'CPU->GPU (ms)':<20} {'GPU->CPU (ms)':<20}\")\n",
    "for size in sizes: \n",
    "    cpu_tensor = torch.randn(size,size)\n",
    "\n",
    "    # Measure CPU > GPU transfer\n",
    "    start = time.time()\n",
    "    gpu_tensor = cpu_tensor.cuda()\n",
    "    torch.cuda.synchronize()\n",
    "    cpu_to_gpu_time = (time.time() - start) * 1000\n",
    "\n",
    "    # Measure GPU to CPU transfer \n",
    "    start = time.time()\n",
    "    back_to_cpu = gpu_tensor.cpu()\n",
    "    gpu_to_cpu_time = (time.time() - start) * 1000\n",
    "\n",
    "    print(f\"{size:<15} {cpu_to_gpu_time:<20.4f} {gpu_to_cpu_time:<20.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4748672f-576a-436c-af6b-578022c177a1",
   "metadata": {},
   "source": [
    "Something is wrong with the above numbers. \n",
    "#TODO\n",
    "I'll come back to these numbers later and dig deeper into them. \n",
    "\n",
    "Let's move on to the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49597079-ba3e-4b2d-acbc-0af7cf000cdb",
   "metadata": {},
   "source": [
    "# Tensor Operations on GPU\n",
    "```markdown\n",
    "Questions to explore\n",
    "1. Do operations require tensors to be on the same device? \n",
    "2. What happens if you mix CPU and GPU tensors? \n",
    "3. How does broadcasting work on GPU? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88298d47-3003-4688-8e59-59d20a49bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors on GPU\n",
    "device = 'cuda'\n",
    "a = torch.randn(3,3, device=device)\n",
    "b = torch.randn(3,3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92bd2e35-4d68-446b-ae31-c884bdd96ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition: \n",
      "Result device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Addition: \")\n",
    "c = a + b \n",
    "print(f\"Result device : {c.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa5f7b8c-666d-4404-bef3-c6f5e6a71e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Multiplication: \n",
      "Result device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix Multiplication: \")\n",
    "c = torch.mm(a,b)\n",
    "print(f\"Result device : {c.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9690463-b90c-4c2f-9da2-4766bae8a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting (tensor + scalar)\n",
      "Result shape: torch.Size([3, 3]), device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting \n",
    "print(\"Broadcasting (tensor + scalar)\")\n",
    "e = a + 5.0\n",
    "print(f\"Result shape: {e.shape}, device: {e.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85670dd-dfe9-4c83-b065-d826a838c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate device mismatch error\n",
    "if torch.cuda.is_available(): \n",
    "    print(f\"\\n Attempting CPU + GPU operation: \")\n",
    "    try: \n",
    "        cpu_tensor = torch.randn(3,3)\n",
    "        gpu_tensor = torch.randn(3,3, device='cuda')\n",
    "        result = cpu_tensor + gpu_tensor\n",
    "    except RuntimeError as e: \n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"All tensors in an operation must be on the same device\")\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
